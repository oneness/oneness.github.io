<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<link rel="alternate"
      type="application/rss+xml"
      href="https://www.birkey.co/rss.xml"
      title="RSS feed for https://www.birkey.co/">
<title>BirkeyCo</title>
<link href="static/style.css" rel="stylesheet" type="text/css" />
	   <link rel="apple-touch-icon" sizes="180x180" href="static/apple-touch-icon.png">
	   <link rel="icon" type="image/png" sizes="32x32" href="static/favicon-32x32.png">
	   <link rel="icon" type="image/png" sizes="16x16" href="static/favicon-16x16.png">
	   <link rel="manifest" href="/site.webmanifest">
	   <link rel="mask-icon" href="static/safari-pinned-tab.svg" color="#5bbad5">
	   <link rel="alternate" type="application/rss+xml" title="RSS Feed for birkey.co" href="/rss.xml">
	   <meta name="author" content="Kasim Tuman">
	   <meta name="referrer" content="no-referrer">
	   <meta name="msapplication-TileColor" content="#da532c">
	   <meta name="theme-color" content="#ffffff">
	   <meta http-equiv="content-type" content="application/xhtml+xml; charset=UTF-8">
	   <meta name="viewport" content="initial-scale=1,width=device-width,minimum-scale=1">
	  </head>
<body>
<div id="preamble" class="status"><br><center>
       <div style="display: inline-block; vertical-align:middle;">
  <a href="https://www.birkey.co/index.html" style="text-decoration: none;"><b>BIRKEY CONSULTING</b><br>
  </a><hr/><div style="text-align: justify;display: inline-block; width: 100%;">
<a class="title" href="https://github.com/oneness">ABOUT</a> &nbsp;<a class="title" href="https://www.birkey.co/rss.xml">RSS</a> &nbsp;<a class="title" href="https://www.birkey.co/archive.html">ARCHIVE</a></div></div>
  </center><br><br>
  <div style="margin-bottom: 3ch;text-transform: none;"></div></div>
<div id="content">

<div class="post-date">02 Aug 2025</div><h1 class="post-title"><a href="https://www.birkey.co/2025-08-02-hacker-news-ai-coding-experience-analysis.html">Hacker news AI coding experience analysis</a></h1>
<p>
I have been using and experimenting with AI coding tools heavily for
the last 3 months or so since joining <a href="https://www.legionhealth.com/">Legion Health</a> as a Founding
Engineer. I was somewhat skeptical and approached using AI with
suspicion since ChatGPT came out. I use Emacs as my workbench and
optimized my workflow around using it as a terminal multiplexer, which
naturally fits with Claude Code that I use as my main programming
assistant. Below is my simple setup that might benefit other fellow
minimalist Emacs users.
</p>
<div class="org-src-container">
<pre class="src src-elisp">(use-package eat :ensure t :config
  (setq eat-term-name "xterm-256color" eat-kill-buffer-on-exit t
        process-adaptive-read-buffering nil eat-term-scrollback-size 500000)
  (define-key eat-semi-char-mode-map [?\s-v] #'eat-yank)
  (define-key eat-semi-char-mode-map [?\C-c ?\C-r] #'k/eat-redisplay))
(defun k/eat-redisplay ()
"Fix eat flicker/flash and display funkiness"
(interactive)
(unless (derived-mode-p 'eat-mode)
  (error "Not in an eat-mode buffer"))
(when (and (boundp 'eat-mode) eat-mode (boundp 'eat-terminal) eat-terminal)
  (let* ((process (eat-term-parameter eat-terminal 'eat--process))
         (window (get-buffer-window (current-buffer))))
    (if (and process (process-live-p process) window)
        (eat--adjust-process-window-size process (list window)))))
(setq-local window-adjust-process-window-size-function
            'window-adjust-process-window-size-smallest)
(goto-char (point-min))
(redisplay)
(goto-char (point-max))
(redisplay)
(setq-local window-adjust-process-window-size-function 'ignore))
</pre>
</div>

<p>
I start an eat shell and run:
</p>
<div class="org-src-container">
<pre class="src src-bash">cd ~/repos/project-x &amp;&amp; claude
</pre>
</div>

<p>
This is a fast moving landscape and I find following few points are
extremely helpful in my workflow:
</p>
<ul class="org-ul">
<li>Spend few minutes to gather up and feed context related to what needs to be
done to Claude at the start of a session.</li>
<li>Always ask to show a plan and instruct Claude for guidance if there
multiple options for a solution.</li>
<li>Provide a skeleton such as directory structure, file names, function names
and signatures.</li>
<li>Provide use case and acceptance criteria testing instructions.</li>
</ul>

<p>
On top of that, you need to make sure Claude has access to tools that
enhances its ability to look up relevant information. To provide more
balanced overview of the AI coding experience, I used a great data
analysis tool for Hacker News called <a href="https://camelai.com/">CamelAI</a>. Below are the result
that more or less resonates with my personal experience.
</p>
<div id="outline-container-orgb608789" class="outline-2">
<h2 id="orgb608789">🏆 Top Stories by Engagement</h2>
<div class="outline-text-2" id="text-orgb608789">
<ul class="org-ul">
<li>Claude 3.7 Sonnet and Claude Code (2,127 points) 🟢
<ul class="org-ul">
<li>Overwhelmingly positive reception for AI coding capabilities</li>
<li>Demonstrates Claude's dominance in the space</li>
</ul></li>
<li>Cursor IDE lockout policy problems (1,511 points) 🔴
<ul class="org-ul">
<li>Major backlash against policy changes causing user cancellations</li>
<li>Shows fragility of user trust in AI tools</li>
</ul></li>
<li>AlphaEvolve: Gemini coding agent (1,036 points) 🚀
<ul class="org-ul">
<li>Google's advanced algorithm design agent</li>
<li>High interest in autonomous coding capabilities</li>
</ul></li>
<li>"Enough AI copilots, we need AI HUDs" (964 points) 🎛️
<ul class="org-ul">
<li>Forward-thinking discussion about UI evolution</li>
<li>Community wants more integrated experiences</li>
</ul></li>
<li>Void: Open-source Cursor alternative (948 points) 🔓
<ul class="org-ul">
<li>Strong demand for open-source alternatives</li>
<li>Privacy and control concerns driving adoption</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org0600797" class="outline-2">
<h2 id="org0600797">📊 Key Trends &amp; Patterns</h2>
<div class="outline-text-2" id="text-org0600797">
<ul class="org-ul">
<li>🎯 Claude Dominance in AI Coding
<ul class="org-ul">
<li>Evidence: Claude 3.7 Sonnet (2,127 pts), consistent praise in experience stories</li>
<li>Insight: Anthropic's Claude has emerged as the clear leader for serious coding work, with developers consistently praising its code quality and reasoning capabilities over competitors</li>
</ul></li>
<li>⚡ Tool Fragmentation &amp; User Frustration
<ul class="org-ul">
<li>Evidence: Cursor problems (1,511 pts), multiple "stopped using AI" stories (365, 109 pts)</li>
<li>Insight: Users are jumping between tools due to policy changes, reliability issues, and unmet expectations. No single tool has achieved universal satisfaction, leading to "tool fatigue"</li>
</ul></li>
<li>🔄 The Productivity Paradox
<ul class="org-ul">
<li>Evidence: "Anyone struggling to get value out of coding LLMs?" (345 pts), productivity studies showing mixed results</li>
<li>Insight: Despite massive hype, many developers struggle to see concrete productivity gains. The "almost right" code problem creates hidden productivity taxes that offset benefits</li>
</ul></li>
<li>🧠 Cognitive Dependency Concerns
<ul class="org-ul">
<li>Evidence: "After months of coding with LLMs, I'm going back to using my brain" (365 pts)</li>
<li>Insight: Growing concern about over-reliance on AI leading to skill atrophy and reduced problem-solving capabilities among developers</li>
</ul></li>
<li>🏢 Enterprise vs Individual Experience Gap
<ul class="org-ul">
<li>Evidence: Microsoft 365 Copilot disaster (602 pts) vs individual success stories</li>
<li>Insight: Stark divide between enterprise rollout failures and individual developer successes. Enterprise context adds complexity that current tools struggle with</li>
</ul></li>
<li>🔓 Open Source Alternative Movement
<ul class="org-ul">
<li>Evidence: Void alternative (948 pts), Tabby self-hosted (366 pts)</li>
<li>Insight: Strong demand for open-source, self-hosted alternatives driven by privacy concerns, cost considerations, and desire for control</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org1de48b7" class="outline-2">
<h2 id="org1de48b7">🎯 Engineer Experience Patterns</h2>
<div class="outline-text-2" id="text-org1de48b7">
<ul class="org-ul">
<li>🟢 Positive Experiences
<ul class="org-ul">
<li>Who: Experienced developers using AI as an enhancement tool</li>
<li>Patterns: Claude-based tools getting consistent praise, terminal-based tools popular with power users</li>
<li>Benefits: Code generation, debugging assistance, learning new patterns</li>
<li>Key Success Factor: Using AI to amplify existing skills, not replace them</li>
</ul></li>
<li>🔴 Negative Experiences
<ul class="org-ul">
<li>Who: Beginners over-relying on AI, enterprise users with complex requirements</li>
<li>Patterns: Policy changes causing churn, productivity promises not materializing, "almost right" code creating more work</li>
<li>Problems: Skill degradation, tool reliability issues, hidden productivity costs</li>
<li>Key Failure Factor: Expecting AI to replace fundamental programming knowledge</li>
</ul></li>
<li>🟡 Mixed Experiences
<ul class="org-ul">
<li>Who: Pragmatic developers experimenting with different approaches</li>
<li>Patterns: Tools working well for specific use cases, steeper learning curve than expected, context-dependent effectiveness</li>
<li>Insight: Success heavily depends on matching use case, experience level, and realistic expectations</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org2b66112" class="outline-2">
<h2 id="org2b66112">📈 Temporal Evolution (2024-2025)</h2>
<div class="outline-text-2" id="text-org2b66112">
<ul class="org-ul">
<li>Early 2024: Initial hype phase - GitHub Copilot going free, new tool launches</li>
<li>Mid 2024: Reality check phase - limitations becoming apparent, user frustrations mounting</li>
<li>Late 2024: Maturation phase - Claude emerges as leader, tool fragmentation increases</li>
<li>Early 2025: Sophistication phase - Claude 3.7/Code dominance, better understanding of limitations</li>
<li>Mid 2025: Pragmatic phase - Focus on specific use cases, open-source alternatives, realistic expectations</li>
</ul>
</div>
</div>
<div id="outline-container-orgf688f98" class="outline-2">
<h2 id="orgf688f98">🎯 Critical Insights for Engineers</h2>
<div class="outline-text-2" id="text-orgf688f98">
<ul class="org-ul">
<li>💪 Skill Foundation is Critical
<ul class="org-ul">
<li>AI tools amplify existing programming skills rather than replace them. Developers who understand fundamentals see the most benefit.</li>
</ul></li>
<li>🎯 Context Matters Enormously
<ul class="org-ul">
<li>Success depends heavily on use case, project complexity, and domain. There's no universal "AI coding works" or "doesn't work."</li>
</ul></li>
<li>🔧 Tool Landscape is Rapidly Changing
<ul class="org-ul">
<li>Claude-based tools currently leading, but the landscape shifts quickly. Expect to try multiple approaches.</li>
</ul></li>
<li>⚠️ Cognitive Risks are Real
<ul class="org-ul">
<li>Over-reliance can lead to skill degradation. Many successful developers use AI selectively while maintaining core problem-solving abilities.</li>
</ul></li>
<li>📊 Productivity Benefits are Mixed
<ul class="org-ul">
<li>Benefits exist but are often not as dramatic as promised. The "almost right" problem creates hidden costs that offset gains.</li>
</ul></li>
<li>🏢 Enterprise Success ≠ Individual Success
<ul class="org-ul">
<li>Individual developer success doesn't guarantee organizational success. Enterprise complexity creates additional challenges.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgd5d7cdc" class="outline-2">
<h2 id="orgd5d7cdc">🔮 Future Outlook</h2>
<div class="outline-text-2" id="text-orgd5d7cdc">
<ul class="org-ul">
<li>🎯 Specialization: Tools becoming more domain-specific and context-aware</li>
<li>🤝 Hybrid Workflows: Combination of AI assistance and traditional coding becoming the norm</li>
<li>🔬 Better Metrics: More sophisticated ways to measure actual productivity impact</li>
<li>🎓 Education Evolution: Teaching AI-assisted development as a core skill</li>
<li>🔓 Democratization: More open-source and self-hosted options emerging</li>
<li>🎛️ UI Innovation: Moving beyond copilots to more integrated experiences (AI HUDs)</li>
</ul>
</div>
</div>
<div id="outline-container-org36da3cc" class="outline-2">
<h2 id="org36da3cc">🔍 Specific Tool Performance</h2>
<div class="outline-text-2" id="text-org36da3cc">
<ul class="org-ul">
<li>🥇 Claude: Clear winner for code quality, reasoning, and complex tasks</li>
<li>🥈 Cursor: Popular but plagued by policy and reliability issues</li>
<li>🥉 GitHub Copilot: Solid mainstream choice, good accessibility for beginners</li>
<li>🔓 Open Source (Void/Tabby): Rising alternatives for privacy/control-conscious developers</li>
<li>⚠️ Enterprise Tools: Microsoft 365 Copilot struggled badly in enterprise rollouts</li>
</ul>
</div>
</div>
<div id="outline-container-org287b962" class="outline-2">
<h2 id="org287b962">💡 Bottom Line</h2>
<div class="outline-text-2" id="text-org287b962">
<ul class="org-ul">
<li>The AI coding experience is highly polarized - it works exceptionally well for some developers in specific contexts, but fails to deliver promised productivity gains for many others. Success requires:
<ul class="org-ul">
<li>Matching the right tool to the right use case</li>
<li>Maintaining realistic expectations</li>
<li>Preserving core programming skills</li>
<li>Understanding tool limitations</li>
<li>Being prepared to adapt as the landscape evolves</li>
</ul></li>
</ul>
</div>
</div>
<div class="taglist"><a href="https://www.birkey.co/tags.html">Tags</a>: <a href="https://www.birkey.co/tag-ai.html">AI</a> </div>
<div class="post-date">25 May 2025</div><h1 class="post-title"><a href="https://www.birkey.co/2025-05-25-mcp-explained-with-code.html">MCP explained with code</a></h1>
<p>
So you are curios about how Model Context Protocol works as a
stand-alone client and server, and as a hub to provide context for LLM
models to do their magic? Then read on.
</p>

<p>
In this post, I am going to write about following few points:
</p>
<ul class="org-ul">
<li>Why MCP?</li>
<li>How does it work?</li>
<li>What makes it worth learning and excited about?</li>
</ul>

<p>
<b>Disclaimer:</b> This blog post is not about making a claim of its merit
 only without its trade offs, which is a blog post for another day and
 another time. So take everything you read here with a grain of salt.
</p>
<div id="outline-container-orge2a41fa" class="outline-2">
<h2 id="orge2a41fa">Why MCP?</h2>
<div class="outline-text-2" id="text-orge2a41fa">
<p>
LLMs needs context to narrow its pattern recognitions so it can
provide more contextual help for the domain you are engaging them
with. MCP provides a plug and play way to do just that. I am not going
to repeat the excellent documentation from its web site <a href="https://modelcontextprotocol.io/introduction">here</a> but I'd
like to drive the point a bit further expanding upon their USB-C
analogy. USB-C is a universal standard (interface to be exact) that every
device manufactures uses so their device can be plugged into a USB
port of a computer. Once a device (think of it as a MCP Server)
connects to USB-C port, it exchanges information with the host about
its capabilities using a USB-C subsystem of your PC (think of it as a
MCP client) so you can communicate with the device. MCP client/server
behaves just like that and we are ready to the next section to see the
actual code so you can run to convince yourself.
</p>
</div>
</div>
<div id="outline-container-org12f4eb8" class="outline-2">
<h2 id="org12f4eb8">How does it work?</h2>
<div class="outline-text-2" id="text-org12f4eb8">
<p>
To understand any concept, I usually take it apart to its smallest
logical units to see them in actions. Below are all you need to run a
stand-alone MCP client and server in TypeScript:
</p>

<div class="org-src-container">
<pre class="src src-TypeScript">// Core bits of Server here. See full code:
// https://github.com/oneness/ts-mcp-client-server/blob/main/src/server.ts
class MCPServer {
  ... // omitted boiler plate code
  private setupToolHandlers() {
    // Handle list_tools requests
    this.server.setRequestHandler(ListToolsRequestSchema, async (): Promise&lt;ListToolsResult&gt; =&gt; {
      return {
        tools: [
          {
            name: "say_hello",
            description: "Says hello to a person",
            inputSchema: {
              type: "object",
              properties: {
                name: {
                  type: "string",
                  description: "The name of the person to greet",
                },
              },
              required: ["name"],
            },
          } as Tool,
          {
            name: "get_time",
            description: "Gets the current time",
            inputSchema: {
              type: "object",
              properties: {},
            },
          } as Tool,
        ],
      };
    });

    // Handle call_tool requests
    this.server.setRequestHandler(CallToolRequestSchema, async (request): Promise&lt;CallToolResult&gt; =&gt; {
      const { name, arguments: args } = request.params;

      switch (name) {
        case "say_hello":
          const personName = args?.name || "World";
          return {
            content: [
              {
                type: "text",
                text: `Hello, ${personName}! This is a greeting from the MCP server.`,
              },
            ],
          };

        case "get_time":
          return {
            content: [
              {
                type: "text",
                text: `Current time: ${new Date().toISOString()}`,
              },
            ],
          };

        default:
          throw new Error(`Unknown tool: ${name}`);
      }
    });
  }
  ... // omitted
}

// Client
// https://github.com/oneness/ts-mcp-client-server/blob/main/src/client.ts
class MCPClient {
  ... // omitted
  async listTools() {
    try {
      const response = await this.client.listTools() as ListToolsResult;

      console.log("Available tools:");
      response.tools.forEach((tool) =&gt; {
        console.log(`- ${tool.name}: ${tool.description}`);
      });

      return response.tools;
    } catch (error) {
      console.error("Error listing tools:", error);
      return [];
    }
  }

  async callTool(name: string, args: any = {}) {
    try {
      const response = await this.client.callTool({
        name,
        arguments: args,
      }) as CallToolResult;

      console.log(`Tool '${name}' response:`);
      response.content.forEach((content) =&gt; {
        if (content.type === "text") {
          console.log(content.text);
        }
      });

      return response;
    } catch (error) {
      console.error(`Error calling tool '${name}':`, error);
    }
  }
 ... //omitted 
}
</pre>
</div>

<p>
As you have noticed, there are few interfaces (functions with args and
return schema ) that you need to implement so client and server can
speak to each other using Request/Response format that they
understand (JSON2.0 RPC if you are curious).
</p>

<p>
Now you understand how a stand-alone MCP client and server communicates
with each other (I highly recommend you clone the repo and run `npm
run mcp` to see it in action), Let us see the actual LLM chat that
uses MCP client to talk to LLM by providing MCP server capabilities as
a context to it. LLM can use the structured data (MCP capabilities)
to determine what MCP server tool it can use to answer user's request,
which in turn MCP client uses to execute the tool. Then the tool
response is provided back to LLM so it can formulate a final response
back to the user. Here is the simple ASCII diagram that visualizes the
flow I am talking about:
</p>

<div class="org-src-container">
<pre class="src src-text">+----------+      +------------+      +-----------------+      +-----+
|   User   |      | MCP Client |      | MCP Server Tool |      | LLM |
+----------+      +------------+      +-----------------+      +-----+
     |                 |                  |                  |
     |                 | 0. Connect &amp; Get |                  |
     |                 |    Capabilities  |                  |
     |                 +-----------------&gt;|                  |
     |                 |&lt;-----------------+                  |
     |                 | (Capabilities now known by Client)  |
     |                 |                  |                  |
     |                 | 1. Setup System  |                  |
     |                 |    Prompt w/ MCP |                  |
     |                 |    Capabilities  |                  |
     |                 +------------------------------------&gt;|
     |                 |                  |                  |
     |                 |                  | (LLM is now      |
     |                 |                  |  aware of tools) |
     |                 |                  |                  |
     |  2. Chat Req.   |                  |                  |
     +----------------&gt;|                  |                  |
     |                 |                  |                  |
     |                 | 3. User Query    |                  |
     |                 |  (subsequent)    |                  |
     |                 +------------------------------------&gt;|
     |                 |                  |                  |
     |                 |                  | 4. Process Query |
     |                 |                  |    &amp; Context     |
     |                 |                  |                  |
     |                 |                  |&lt;-----------------| (Tool identified?)
     |                 |                  | 5. Tool Exec.    |
     |                 |                  |    Request       |
     |                 |&lt;------------------------------------|
     |                 |                  |                  |
     |                 | 6. Execute Tool  |                  |
     |                 +-----------------&gt;|                  |
     |                 |                  |                  |
     |                 |&lt;-----------------+                  |
     |                 | 7. Tool Output   |                  |
     |                 |                  |                  |
     |                 | 8. Tool Output   |                  |
     |                 +------------------------------------&gt;|
     |                 |                  |                  |
     |                 |                  |                  |
     |                 |                  |                  |
     |                 |&lt;------------------------------------|
     |                 |                  | 9. Final Response|
     |&lt;----------------+                  |                  |
     | 10. Chat Resp.  |                  |                  |


</pre>
</div>

<p>
Here is the code that shows above flow in action:
</p>

<div class="org-src-container">
<pre class="src src-TypeScript">// Omitted for brevity. See below link for details
// https://github.com/oneness/ts-mcp-client-server/blob/main/src/llm.ts#L69
async processMessage(userMessage: string): Promise&lt;string&gt; {
  console.log(`\n🤖 Processing: "${userMessage}"`);

  // Add user message to conversation
  this.conversationHistory.push({
    role: "user",
    content: userMessage
  });

  try {
    // Prepare tools for Claude
    const tools = this.convertMCPToolsToAnthropicFormat();

    // Get Claude's response
    const response = await this.anthropic.messages.create({
      model: "claude-3-5-sonnet-20241022",
      max_tokens: 1000,
      system: this.systemPrompt,
      messages: this.conversationHistory,
      tools: tools.length &gt; 0 ? tools : undefined,
    });

    console.log(`🧠 Claude response:`, JSON.stringify(response, null, 2));

    // Process the response
    let finalResponse = "";
    const toolResults: MCPToolResult[] = [];

    // Handle different content types
    for (const content of response.content) {
      if (content.type === 'text') {
        finalResponse += content.text;
      } else if (content.type === 'tool_use') {
        console.log(`🔧 Claude wants to use tool: ${content.name} with args:`, content.input);

        // Call the MCP tool
        const mcpResult = await this.mcpClient.callTool(content.name, content.input);

        let toolResultText = "No result";
        if (mcpResult &amp;&amp; mcpResult.content) {
          toolResultText = mcpResult.content
            .filter(c =&gt; c.type === 'text')
            .map(c =&gt; c.text)
            .join('\n');
        }

        toolResults.push({
          tool: content.name,
          result: toolResultText
        });

        // Add tool result to conversation for Claude's next response
        this.conversationHistory.push({
          role: "assistant",
          content: response.content
        });

        this.conversationHistory.push({
          role: "user",
          content: [
            {
              type: "tool_result",
              tool_use_id: content.id,
              content: toolResultText
            }
          ]
        });
      }
    }

    // If we used tools, get Claude's final response incorporating the results
    if (toolResults.length &gt; 0) {
      console.log(`📊 Tool results:`, toolResults);

      const finalCompletion = await this.anthropic.messages.create({
        model: "claude-3-5-sonnet-20241022",
        max_tokens: 1000,
        system: this.systemPrompt,
        messages: this.conversationHistory,
        tools: tools.length &gt; 0 ? tools : undefined,
      });

      // Extract text from final response
      finalResponse = "";
      for (const content of finalCompletion.content) {
        if (content.type === 'text') {
          finalResponse += content.text;
        }
      }

      this.conversationHistory.push({
        role: "assistant",
        content: finalCompletion.content
      });
    } else {
      // No tools were used, add the response to history
      this.conversationHistory.push({
        role: "assistant",
        content: response.content
      });
    }

    return finalResponse;

  } catch (error) {
    console.error('Error calling Claude:', error);
    return "I'm sorry, I encountered an error processing your request.";
  }
}
</pre>
</div>
</div>
</div>
<div id="outline-container-org0e026ce" class="outline-2">
<h2 id="org0e026ce">What makes it worth learning and excited about?</h2>
<div class="outline-text-2" id="text-org0e026ce">
<p>
If you have been following LLM landscape, you might have come to a
realization that most of us (unless you are an AI researcher) are in
the business of providing the most accurate and up to date context to
the foundational LLM models. MCP unifies the way LLM models, Consumer
Applications (clients) and Resource Providers (servers) communicates
thus reducing M*N integration issues to M+N, which is worth learning,
implementing and being excited about. Even without the context of LLM,
I hope more data or service providers exposes their system
capabilities by implementing MCP server contracts. That would
dramatically reduce so much wasted time spent on ad-hoc integration
API glue code.
</p>

<p>
Hope you learned a thing or two about MCP. Keep learning and have fun!
</p>
</div>
</div>
<div class="taglist"><a href="https://www.birkey.co/tags.html">Tags</a>: <a href="https://www.birkey.co/tag-mcp.html">mcp</a> </div>
<div class="post-date">15 Apr 2025</div><h1 class="post-title"><a href="https://www.birkey.co/2025-04-15-nix:-better-way-for-fun-and-profit.html">Nix: Better way for fun and profit</a></h1>
<p>
Nix is started in 2003 as a research project aimed to solve the
problem of <b>reliable software deployment</b>. The PhD thesis titled <b>The
Purely Functional Software Deployment Model</b> proposed a novel way of
building software where the final artifact is purely dependent on the
inputs to the build system, which is a pure function in a mathematical
sense. Regardless of where you are in your nix journey, I can't
recommend this paper (<a href="https://edolstra.github.io/pubs/phd-thesis.pdf">thesis</a>) enough. It is very approachable and
worth a read so you learn from first principle of what, why and how
about Nix.
</p>

<p>
Nix is a software build and management system that can replace
traditional package managers, build environments and configuration
tools. Due to the inherent complexity of the problem domain nix is
designed to solve and its long history, it has pretty steep learning
curve but not unsurmountable. One of the common point of confusions is
how the term `Nix` is used in documentations, tutorials and
blogosphere. So let me clarify few terminologies that often gets
overloaded. 
</p>

<ul class="org-ul">
<li><b>Nix:</b> Unless otherwise fully qualified, I use it to mean the
software build and management system.</li>
<li><b>Nix CLI:</b> The nix command line client that one uses to interact with nix.</li>
<li><b>Nix DSL:</b> The domain specific language that nix uses to generate a
software package. I would like to see everyone start using it to
mean the nix language.</li>
<li><b>Flakes:</b> A Nix DSL with number of conventions that is designed to
ease the configuration and discoverability of software packaging lifecycle.</li>
<li><b>NixOS:</b> The final artifact, which is happened to be a Linux
Operating System that is generated by feeding Nix DSL to nix. I will
not be covering it in this blog post.</li>
</ul>

<p>
After few false starts and restarts, below are what I believe to be
better ways for getting started, using nix for fun and profit.
</p>
<div id="outline-container-org2f32676" class="outline-2">
<h2 id="org2f32676">Installation</h2>
<div class="outline-text-2" id="text-org2f32676">
<p>
I have a following bash script to install a specific version so I can
have control over which version to install, what features enable and
disable.
</p>

<div class="org-src-container">
<pre class="src src-bash">#!/usr/bin/env bash
set -Eeuo pipefail

VERSION='2.28.1' # replace it with the latest version
URL="https://releases.nixos.org/nix/nix-${VERSION}/install"
MY_CONF="$HOME/.dotfiles/nix/nix.conf"
sh &lt;(curl --location "${URL}") \
     --daemon \
     --no-channel-add \
     --nix-extra-conf-file ${MY_CONF}
# conf file has this content
experimental-features = nix-command flakes
</pre>
</div>
<p>
The `&#x2013;no-channel-add` and the extra conf file needs some
explanation. Nix called a remote url a channel that gets automatically
installed, where nix uses to retrieve package definitions (Nix DSL) to
manage packages. It introduces a state, which is currently installed
channel url that is outside of Nix DSL, thus defeating the purpose of
reproducibility. It is considered legacy feature and not needed by
flakes, an experimental feature already widely adopted by the
community. So I highly recommend enabling flakes and additional
commands to interact with it.
</p>
</div>
</div>
<div id="outline-container-org2b5adb4" class="outline-2">
<h2 id="org2b5adb4">Using for fun and sanity</h2>
<div class="outline-text-2" id="text-org2b5adb4">
<p>
Every project depends on existing software that is beyond your
control. Nix DSL enables you to declaratively specify your projects
dependencies, a repo or a tar-ball down to the file digest of its
content, which is what gives nix superpowers of being a deterministic
and reproducible package manager. This means that if your inputs stays
the same, nix guarantees that it produces the exact same output
regardless of when and where. Below is a flake that pulls in latest
version of Clojure into your project.
</p>

<div class="org-src-container">
<pre class="src src-nix">{
  # optional attribute
  description = "My awesome Clojure/ClojureScript project";

  # required attribute
  inputs = {
    # nix dsl fns useful for writing flakes
    flake-utils.url = "github:numtide/flake-utils/v1.0.0";
    # Pins state of the packages to a specific commit sha
    pinnedPkgs.url = "github:NixOS/nixpkgs/c46290747b2aaf090f48a478270feb858837bf11";
  };

  # required attribute
  outputs = { self, flake-utils, pinnedPkgs }@inputs :
  flake-utils.lib.eachDefaultSystem (system:
  let pinnedSysPkgs = inputs.pinnedPkgs.legacyPackages.${system};
  in
  {
    devShells.default = pinnedSysPkgs.mkShell {
      packages = [
        pinnedSysPkgs.clojure
      ];

      # commands to run in the development interactive shell
      shellHook = ''
        echo To get Clojure REPL, Run:
        echo clojure
        echo To get ClojureScript REPL, Run:
        echo clj -Sdeps \'{:deps {org.clojure/clojurescript {:mvn/version "1.11.132"}}}\' -M -m cljs.main --repl
      '';
    };
    packages = {
      docker = pinnedSysPkgs.dockerTools.buildLayeredImage {
        name = "My awesome Clj docker image built by nix";
        tag = "latest";
        contents = [pinnedSysPkgs.clojure];
      };
    };
  });
}

</pre>
</div>

<p>
Do not worry too much about not understanding above nix dsl code. The
most important thing to know is that it is nix dsl referred to as a
flake that specifies its inputs and outputs declaratively. Save above
code as `flake.nix`, which is a convention, then run `nix develop` to
get an interactive shell with Clojure in your path. Nix can do way
more than this. However, I recommend you just start with solving
project dependencies problem. Above flake gives you following
benefits:
</p>
<ul class="org-ul">
<li>Ability to pin the exact versions of your project dependencies.</li>
<li>Cross platform development environment that works
both in MacOS and various flavors of Linux.</li>
<li>Determinate and reproducible development environment that
eliminates "it works on my machine" tooling issues.</li>
</ul>
<p>
One important thing to notice here is the way I chose to reference
the url inputs of the flake. I deliberately used tags or commit sha to
prevent the state of the urls (thus the state of the nix DSL) change
under me, which defeats the purpose of having a determinate
and reproducible way to get a development environment. I have
following bash script that prints available tags and corresponding
commit hash:
</p>
<div class="org-src-container">
<pre class="src src-bash"> git_tag_sha () {
   repo="$1"
   echo "********************************************************"
   echo "Available release and commit sha for pinning are:"
   echo "********************************************************"
   printf "\033[1m%-12s %s\033[0m\n" "release" "commit sha"
   curl -s https://github.com/$repo/tags | grep -oP 'href="\K[^"]*(releases/tag|nixpkgs/commit)[^"]*' | awk -F '/' 'NR%2{tag=$NF; next} {printf "%-12s %s\n", tag, $NF}'
   echo
   echo "****************************************************************************"
   echo "Please replace the commit sha of following line to pin pkgs to a commit sha: "
   echo "pinnedPkgs.url = github:$repo/&lt;commit&gt;"
   echo "****************************************************************************"
   echo
}
# You can run it like this:
 git_tag_sha "NixOS/nixpkgs"
</pre>
</div>
</div>
</div>
<div id="outline-container-orgec3469a" class="outline-2">
<h2 id="orgec3469a">Profiting in CI/CD and production</h2>
<div class="outline-text-2" id="text-orgec3469a">
<p>
This is probably one of the most frictionless and rewarding outcome of
using nix. Nix is designed to solve the problem of software deployment
after all but the wholesale adoption in production might prove to be
too much for the final gain. To spare yourself countless hours of
frustration, I highly recommend you start with using it to build
docker image if you happened to use docker and Kubernetes. Nix has
superb built-in support for making the smallest possible docker image
otherwise impossible. Above flake already includes `docker` image as
one of its packages output. Here is how you build and load the docker image:
</p>

<div class="org-src-container">
<pre class="src src-bash">nix build .#docker # the image will be in ./result
docker load &lt; ./result # to get it ready to be deployed
</pre>
</div>

<p>
It is a declarative way (using the power of Nix DSL compared to using
series commands in YAML file) to deterministically reproduce layered
Docker image that saves time and money in your DevOps journey. Have
fun and enjoy!
</p>
</div>
</div>
<div class="taglist"><a href="https://www.birkey.co/tags.html">Tags</a>: <a href="https://www.birkey.co/tag-nix.html">nix</a> </div>
<div class="post-date">18 Jun 2023</div><h1 class="post-title"><a href="https://www.birkey.co/2023-06-18-google-bard-and-emacs.html">Google Bard and Emacs</a></h1>
<p>
After reading a Google blog post on Bard's increasing ability for
reasoning about source code, I thought I would give it a try. The
issue is that not like OpenAI, Bard currently does not have an http
API that I can use via curl. I googled around and came across the
`bard-rs` project here: <a href="https://github.com/Alfex4936/Bard-rs">https://github.com/Alfex4936/Bard-rs</a>. So I
followed the excellent instruction to get set up using bard from
command line and its is pretty solid. I used following Elisp to use
`bard-rs` from Emacs' compilation buffer here:
</p>
<div class="org-src-container">
<pre class="src src-emacs-lisp">(defun kcompilation-start (cmd name &amp;optional mode)
  (let* ((compile-command nil)
         (compilation-save-buffers-predicate 'ignore)
         (compilation-buffer
          (compilation-start cmd
                             (if (equal mode 'read-only) nil t)
                             (lambda (m)
                               (or (when (boundp 'name)
                                     (format "*%s*" name))
                                   (buffer-name))))))
    (when current-prefix-arg
      (with-current-buffer compilation-buffer
        (switch-to-prev-buffer (get-buffer-window (current-buffer)))))
    (message (format "Running %s in %s ..." cmd name))))

(defun kprompt-bard (&amp;optional p)
  "Prompts for input to send it to `bard` using `bard-rs` in
*bard-prompt* buffer. If mark-active, uses the text in the region
 as the prompt"
  (interactive "P")
  (let* ((bs "bard-prompt")
         (bname (format "*%s*" bs))
         (bname (if (get-buffer bname)
                    bname
                  (progn (kcompilation-start "bard-rs -e ~/.env" bs)
                         bname)))
         (prompt (if mark-active
                     (replace-regexp-in-string
                      "\n"
                      ""
                      (buffer-substring-no-properties (region-beginning) (region-end)))
                   (read-string "AI Chat Prompt: "))))
    (with-current-buffer (pop-to-buffer bname)
      (when p
        (end-of-buffer)
        (insert "!reset")
        (comint-send-input)
        (end-of-buffer)
        (insert prompt)
        (comint-send-input))
      (when (not p)
        (end-of-buffer)
        (insert prompt)
        (comint-send-input)))))
</pre>
</div>

<p>
You can bind `kprompt-bard` to any key of your choice and start
interacting with Google bard from the comfort of Emacs' buffer. 
</p>
<div class="taglist"><a href="https://www.birkey.co/tags.html">Tags</a>: <a href="https://www.birkey.co/tag-ai.html">AI</a> <a href="https://www.birkey.co/tag-emacs.html">emacs</a> </div>
<div class="post-date">14 May 2023</div><h1 class="post-title"><a href="https://www.birkey.co/2023-05-14-ai-or-not-to-ai.html">AI or not to AI</a></h1>
<p>
1913 Webster dictionary gives following definition to Artificial Intelligence:
<b>Artificial</b> - 1. Made or contrived by art; produced or modified by
human skill and labor, in opposition to natural; 2. Feigned;
fictitious; assumed; affected; not genuine. 3. Artful; cunning;
crafty. 4. Cultivated; not indigenous; not of spontaneous growth;
<b>Intelligence</b> - 1. The act or state of knowing; the exercise of the
understanding. 2. The capacity to know or understand; readiness of
comprehension; the intellect, as a gift or an endowment. 3. Knowledge
imparted or acquired, whether by study, research, or experience;
general information. Specifically; (Mil.) Information about an enemy
or potential enemy, his capacities, and intentions.
</p>

<p>
Let us read and re-read above definitions and give it a few minutes to
sink in. With our skilled labor, we have managed to produce a very
powerful fictitious software that can understand, produce and reason
about human generated artifacts such as language, images and
videos. Our ability to make it more scalable, more accurate and more
controllable is just a matter of time considering what we have is
already pretty powerful enough. My following interaction with ChatGPT
3.5 is already impressive:
</p>
<div class="org-src-container">
<pre class="src src-emacs-lisp">[ME]: Explain, then add tests and simplify if possible:
(defun kprompt (&amp;optional p)
  (interactive "P")
  (let* ((bname (if p "*org-ai-on-region*" "*scratch*"))
         (directive "Explain, then add tests and simplify if possible:\n")
         (prompt (if p
                     (buffer-substring (region-beginning) (region-end))
                   (read-string "AI Chat Prompt: "))))
    (with-current-buffer (pop-to-buffer bname)
      (when p
        (erase-buffer)
        (org-mode)
        (setq truncate-lines nil)
        (setq word-wrap t)
        (org-ai--insert-created-timestamp)
        (insert "\n")
        (insert "#+begin_ai\n" "[ME]: " directive prompt "\n" "#+end_ai\n")
        (forward-line -1)
        (org-ai-complete-block))
      (when (not p)
        (beginning-of-buffer)
        (open-line 2)
        (org-ai-prompt prompt)))))

[AI]: This function takes an optional argument, p, which determines whether the user
will be prompted to enter a text or whether the function will use the text highlighted
in the buffer. 

If p is provided, it will create a new buffer with the name "*org-ai-on-region*" and
insert the highlighted text from the current buffer along with a directive. This new
buffer buffer will be formatted using org-mode to allow for better organization of
comprehension text. Finally, it will call org-ai-complete-block to add some
AI-generated text to the prompt.

If p is not provided, it will prompt the user to enter text and then call
org-ai-prompt org-ai-prompt to add some AI-generated text to the prompt.

Tests:

Input:
(kprompt)
Expected Output:
AI Chat Prompt: 

Input:
(kprompt t)
Expected Output:
*org-ai-on-region* buffer created with prompt

Input:
(Highlight "This is a test")
(kprompt t)
Expected output:
*org-ai-on-region* buffer created with "This is a test" text and AI-generated text.

Simplified Code:
The code appears to be relatively straightforward and efficient.
No simplification is needed.
</pre>
</div>

<p>
It understood and reasoned about a piece of code (simple for now but
does not matter as it can get pretty powerful in no time) despite the
fact that it has never seen above code before in its entirety (I am
pretty sure about that since it is a private code and this is the only
time I am making the code public). This degree of intelligence
codified into a software that a few powerful cooperation currently
dominates should be something that keeps every software engineers
awake at night. Not because of its inherent danger or tremendous
productivity boosting ability as folks on the opposite side of
spectrum of current AI debate claimed, but because of the very fact
that every aspect of human lives will be effected by a such a
powerful code like ChatGPT whether we like it or not, and we need
to do whatever we can to ensure it is used for the good of humanity in
general. It is created by humans and should serve humans. Make no
mistake about it. Powerful software systems like that is already used
by big cooperations and rouge states to cajole people into a state of
self censorship if not into a state of heedlessness of its future
implication. Social media, powerful tracking and image recognition
systems are already pervasive in the lives of millions of people that
are being controlled by dictators all around the world (and it is
being exported very actively in the name of economic progress) to
socially engineer people's behaviors that benefits their agenda in
the name of social and economic progress at the very expense of
destroying anyone or anything that is deemed as an obstacle.
</p>

<p>
As a software engineer who have seen the worst of what bad actors can
do with such a powerful systems, I am calling out to all of my fellow
engineers to start thinking about what kind of world we would like our
kids to inherit from us regardless of where you are, who you are and
what is your geopolitical affiliation is. The wave is already there,
and it takes all of us to make sure we are not being social
engineered out of our humanity. I believe in the power of our humanity
to make AI to work for us not the other way around. I registered the
domain <b>www.codeforhumanrights.org</b> few years ago and this might be a
good time to start putting it to a good use. If you are reading this
and feel the need to start doing something, reach out to me via ktuman
at acm dot org.
</p>
<div class="taglist"><a href="https://www.birkey.co/tags.html">Tags</a>: <a href="https://www.birkey.co/tag-ai.html">AI</a> <a href="https://www.birkey.co/tag-emacs.html">emacs</a> </div><div id="archive">
<a href="https://www.birkey.co/archive.html">Other posts</a>
</div>
</div>
<div id="postamble" class="status"><br><center>
	      <div style="display: inline-block; vertical-align:middle;"></div></div>
</body>
</html>
